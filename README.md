# Comparative-Analysis-of-Ada-Hessian-and-1st-Order-Optimizers-for-CSI-Based-Sign-Language-Recognition



This repository contains the implementation and experimental analysis of **Ada-Hessian**, a second-order optimization method, compared against several **first-order optimizers** for **CSI-based sign language recognition**.

The project was completed for \*\*AMAT 591: Optimization Methods and Nonlinear Programming (Spring 2025)\*\* at the \*\*University at Albany\*\*.





###### **üìå Project Overview**



First-order optimizers such as SGD and Adam dominate deep learning due to their efficiency but are often highly sensitive to hyperparameter choices. This project investigates whether incorporating \*\*second-order curvature information\*\* via \*\*Ada-Hessian\*\* can improve convergence speed, stability, and robustness when training neural networks on \*\*WiFi CSI-based sign language data\*\*.



**The study includes:**



\* An implementation of \*\*Ada-Hessian using Hutchinson‚Äôs method\*\* for Hessian diagonal approximation

\* \*\*Spatial (block-wise) averaging\*\* to reduce stochastic curvature noise

\* Extensive comparison with \*\*SGD, Adam, AdamW, Adamax, Nadam, and RMSprop\*\*

\* Evaluation on multiple CSI datasets, including single-user and multi-user settings





\## üìÅ Repository Structure



```

‚îú‚îÄ‚îÄ code/

‚îÇ   ‚îú‚îÄ‚îÄ models/                # CNN-based architectures for CSI classification

‚îÇ   ‚îú‚îÄ‚îÄ optimizers/            # Ada-Hessian and first-order optimizer implementations

‚îÇ   ‚îú‚îÄ‚îÄ training/              # Training and evaluation scripts

‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Helper functions and preprocessing utilities

‚îÇ

‚îú‚îÄ‚îÄ data/

‚îÇ   ‚îî‚îÄ‚îÄ Home/            # Subset of the Home CSI dataset (for demonstration)

‚îÇ

‚îú‚îÄ‚îÄ results/

‚îÇ   ‚îú‚îÄ‚îÄ plots/                  # Training/validation curves

‚îÇ   ‚îú‚îÄ‚îÄ logs/                   # Experiment logs for all configurations

‚îÇ   ‚îî‚îÄ‚îÄ tables/                 # Accuracy and performance comparison tables

‚îÇ

‚îú‚îÄ‚îÄ deliverables/

‚îÇ   ‚îú‚îÄ‚îÄ project\_report.pdf      # Final project report

‚îÇ   ‚îú‚îÄ‚îÄ project\_proposal.pdf    # Initial project proposal

‚îÇ   ‚îî‚îÄ‚îÄ poster.pdf              # Project poster

‚îÇ

‚îú‚îÄ‚îÄ materials/

‚îÇ   ‚îú‚îÄ‚îÄ signfi\_paper.pdf        # SignFi dataset reference paper

‚îÇ   ‚îî‚îÄ‚îÄ adahessian\_paper.pdf    # Ada-Hessian original paper

‚îÇ

‚îî‚îÄ‚îÄ README.md

```



---



\## üß† Problem Formulation



The learning objective is to minimize a non-convex empirical risk function:



\[

\\min\_{\\theta} ; L(\\theta) = \\frac{1}{N}\\sum\_{i=1}^{N} \\ell(x\_i, y\_i; \\theta)

]



While first-order methods rely only on gradient statistics, \*\*Ada-Hessian preconditions gradients using an approximate inverse Hessian\*\*, enabling curvature-aware updates that adapt to the geometry of the loss surface.



---



\## ‚öôÔ∏è Methodology



\### Ada-Hessian Key Components



\* \*\*Hessian diagonal approximation\*\* via Hutchinson‚Äôs method

\* \*\*Block-wise spatial averaging\*\* of curvature estimates

\* \*\*Momentum-based smoothing\*\*, similar to Adam

\* Tunable \*\*Hessian power parameter (k)\*\* to interpolate between gradient descent and Newton-like behavior



\### Neural Network Architecture



\* CNN-based classifier for CSI tensors of shape `(200 √ó 60 √ó 3)`

\* Convolution + BatchNorm + ReLU

\* Average pooling and dropout

\* Fully connected layer with softmax activation



---



\## üìä Experimental Setup



\### CSI Datasets



| Dataset | # Signs | Repetitions | # Instances |

| ------- | ------- | ----------- | ----------- |

| Home    | 276     | 10          | 2,760       |

| Lab     | 276     | 20          | 5,520       |

| Lab150  | 150     | 10          | 7,500       |



> ‚ö†Ô∏è Due to size constraints, \*\*only a subset of the Home dataset\*\* is included in this repository under `data/`.



---



\### Training Configuration



\* Batch size: 256

\* Epochs: up to 300 (Ada-Hessian typically converges within ~50 epochs)

\* Weight decay: (5 \\times 10^{-4})

\* Learning rate:



&nbsp; \* First-order optimizers: 0.01

&nbsp; \* Ada-Hessian: 0.15

\* Learning rate decay at epochs 80, 160, and 240



---



\## üìà Results Summary



\* \*\*Ada-Hessian converges significantly faster\*\* than first-order optimizers

\* Demonstrates \*\*smooth and stable training behavior\*\*

\* Achieves \*\*highest validation accuracy on the Lab dataset\*\*

\* Performance is competitive with \*\*AdamW\*\*, the strongest first-order baseline



Detailed results for \*\*all configurations\*\* (learning rates, Hessian power values, weight decay settings) are available in the `results/` directory.



---



\## ‚è±Ô∏è Computational Trade-offs



\* Ada-Hessian incurs \*\*3‚Äì5√ó higher training time\*\* due to Hessian estimation

\* However, it \*\*requires far fewer epochs to converge\*\*, reducing tuning effort

\* Particularly effective in \*\*high-variability and multi-user CSI settings\*\*



---



\## ‚úÖ Conclusions



\* Ada-Hessian is a \*\*robust and efficient optimizer\*\* for CSI-based sign language recognition

\* Less sensitive to learning rate selection than first-order methods

\* Weight decay remains an important hyperparameter

\* Demonstrates the practical benefits of second-order information in real-world deep learning tasks



---



\## üìö References



1\. \*\*Ma et al.\*\* \*SignFi: Sign Language Recognition Using WiFi\*. ACM IMWUT, 2018.

2\. \*\*Yao et al.\*\* \*AdaHessian: An Adaptive Second Order Optimizer for Machine Learning\*. AAAI, 2021.



---



\## üôè Acknowledgments



\* SignFi dataset and prior CSI-based sign language recognition research

\* \*\*Dr. Zi Yang\*\*, AMAT 591

\* \*\*Dr. Hafiz Imtiaz\*\* and \*\*Dr. Tahsina Farah Sanam\*\* for earlier guidance during undergraduate research



---



If you want, I can next:



\* Add a \*\*Quick Start / How to Run\*\* section

\* Create a \*\*minimal README\*\* version for public release

\* Help you write a \*\*GitHub release description or project tagline\*\*



